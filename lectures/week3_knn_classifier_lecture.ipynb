{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e08132f",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Classification – Lecture Notebook\n",
    "\n",
    "This notebook demonstrates how to use **K-Nearest Neighbors (KNN)** for a **binary classification**\n",
    "task using the IBM Employee Attrition dataset.\n",
    "\n",
    "We will walk through:\n",
    "1. Preparing predictors and target variables  \n",
    "2. Scaling features for distance-based learning  \n",
    "3. Training KNN classifiers with different values of *k*  \n",
    "4. Visualizing decision boundaries (2D projection)  \n",
    "5. Evaluating model performance  \n",
    "\n",
    "> **Key idea:**  \n",
    "> KNN classifies observations based on *distance in predictor space*.  \n",
    "> Feature scaling and encoding choices directly affect which points are considered “neighbors”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be895a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ibm_attrition.csv\")\n",
    "\n",
    "# Dropping columns with no significant contribution.\n",
    "df.drop(columns=[\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\"], inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf418796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of Attrition\n",
    "df[\"Attrition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e707687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target (\"Attrition\")\n",
    "\n",
    "\n",
    "# [1, 0] = [\"Yes\", \"No\"]\n",
    "label_map = {\"No\": 0, \"Yes\": 1}\n",
    "df[\"Attrition\"] = df[\"Attrition\"].map(label_map)\n",
    "\n",
    "\n",
    "# Or use LabelEncoder()\n",
    "# target_encoder = LabelEncoder()\n",
    "# df[\"Attrition\"] = target_encoder.fit_transform(df[\"Attrition\"])\n",
    "\n",
    "# Sanity check to see if we still get the expected counts.\n",
    "df[\"Attrition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16348c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"categorical columns that will be dropped:\")\n",
    "print(categorical_cols)\n",
    "df.drop(columns=categorical_cols, inplace=True)\n",
    "\n",
    "print(f\"New df shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Attrition\"]\n",
    "\n",
    "X = df.drop(columns=[\"Attrition\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test sets. Note that we split data before scaling to prevent any leakage from\n",
    "# test set to our training process.\n",
    "# Also note the use of stratification.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=2026, stratify=y\n",
    ")\n",
    "\n",
    "for v in [X_train, X_test, y_train, y_test]:\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82867709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling training and test set. Note that we ONLY use training set for calculating scaling parameters.\n",
    "\n",
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - train_mean) / train_std\n",
    "X_test_scaled = (X_test - train_mean) / train_std\n",
    "\n",
    "# Alternatively you can use StandardScaler() class which does the same thing.\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train_scaled = scaler.transform(X_train)\n",
    "# # You could also call .fit_transform() method once instead of two steps above.\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# We use np.isclose() and np.allclose() methods to account for computation precision.\n",
    "# the actual values are very close to 0 but not exactly 0.\n",
    "print(np.allclose(X_train_scaled.mean(axis=0), 0))\n",
    "print(np.allclose(X_train_scaled.std(axis=0), 1))\n",
    "print(\"\\n\")\n",
    "\n",
    "# You can see that unlike training data,test data is not exactly standardized to mean of 0 and std of 1.\n",
    "# This is expected as we used training dataset for calculating standardization parameters.\n",
    "print(np.allclose(X_test_scaled.mean(axis=0), 0))\n",
    "print(np.allclose(X_test_scaled.std(axis=0), 1))\n",
    "X_test_scaled.columns[0]\n",
    "print(\"\\n\")\n",
    "\n",
    "# The use of stratification during splitting results in train and test splits with almost\n",
    "# equal distribution on target (y).\n",
    "print(f\"Average for y_train: {y_train.mean():.3f} and for y_test: {y_test.mean():.3f}\")\n",
    "print(np.isclose(y_train.mean(), y_test.mean(), atol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A heatmap provides a good way of understanding multicollinearity between features.\n",
    "# This helps with feature selection.abs\n",
    "# Note that we are not using the test set to avoid leaking problem. \n",
    "\n",
    "# We first assemble back the training set by combining X and y values previously split.\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Then we calculate the Pearson correlation.abs\n",
    "# Note tha Pearson correlation is location and scale invariant.abs\n",
    "# You could take the correlation of non-scaled or scaled data and they will be equal.\n",
    "corr = train_df.corr().round(2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(train_df.drop(columns=\"Attrition\").corr().round(1), annot=True, square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45211475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and storing the pairs of feaures that are highly correlated.\n",
    "multicol_threshold = 0.5\n",
    "\n",
    "mask = np.triu(np.ones(corr.shape), k=1).astype(bool)  # upper triangle, no diagonal\n",
    "pairs = corr.where(mask).stack().reset_index()\n",
    "pairs.columns = [\"feat1\", \"feat2\", \"corr\"]\n",
    "pairs = pairs[pairs[\"corr\"].abs() >= multicol_threshold].sort_values(by=\"feat1\")\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out and storing features with high correlation to target (y).\n",
    "high_cor_threshold = 0.1\n",
    "\n",
    "highly_corr = corr.loc[corr[\"Attrition\"].abs() >= high_cor_threshold, \"Attrition\"]\n",
    "highly_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69170fca",
   "metadata": {},
   "source": [
    "We select the final features we want to include based on previous steps. We want features that are not highly correlated but also potentially related to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"TotalWorkingYears\", \"YearsInCurrentRole\", \"YearsAtCompany\"]\n",
    "extras = [\n",
    "    \"JobSatisfaction\",\n",
    "    \"PerformanceRating\",\n",
    "    \"RelationshipSatisfaction\",\n",
    "    \"WorkLifeBalance\",\n",
    "]\n",
    "retained_features = [f for f in highly_corr.index.to_list() if f not in to_remove]\n",
    "retained_features.extend(extras)\n",
    "retained_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot gives us a good indication of differential class distribution within each feature.\n",
    "\n",
    "sns.pairplot(train_df[retained_features], hue=\"Attrition\", height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_features.remove(\"Attrition\")\n",
    "\n",
    "X_train_retained = X_train_scaled[retained_features]\n",
    "X_test_retained = X_test_scaled[retained_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c824486",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 5, 7, 11, 15, 25, 50]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_retained, y_train)\n",
    "\n",
    "    train_pred = knn.predict(X_train_retained)\n",
    "    test_pred = knn.predict(X_test_retained)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_train, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test, test_pred))\n",
    "\n",
    "    # Alternatively instead of .predict() and accuracy_score() you could do it in one step using .score()\n",
    "    # train_accuracies.append(knn.score(X_train_retained, y_train))\n",
    "    # test_accuracies.append(knn.score(X_test_retained, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d63e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, train_accuracies, marker=\"o\", label=\"Train Accuracy\")\n",
    "plt.plot(k_values, test_accuracies, marker=\"o\", label=\"Test Accuracy\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Neighbors (K)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Accuracy vs K\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaa637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(ax, X, y, k, feature_names, class_labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 300),\n",
    "        np.linspace(y_min, y_max, 300),\n",
    "    )\n",
    "\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    ax.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=y,\n",
    "        cmap=\"coolwarm\",\n",
    "        edgecolor=\"k\",\n",
    "        s=40,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"{feature_names[0]} (Scaled)\")\n",
    "    ax.set_ylabel(f\"{feature_names[1]} (Scaled)\")\n",
    "    ax.set_title(f\"KNN Decision Boundary (k={k})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 3, 5, 7, 15, 25, 50]\n",
    "selected_features = [\"JobInvolvement\", \"Age\"]\n",
    "X_2d = df[selected_features]\n",
    "\n",
    "scaler_2d = StandardScaler()\n",
    "\n",
    "X_2d_scaled = scaler_2d.fit_transform(X_2d)\n",
    "\n",
    "n_rows = (len(k_values) + 1) // 2\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, k in zip(axes, k_values):\n",
    "    plot_decision_boundary(\n",
    "        ax=ax,\n",
    "        X=X_2d_scaled,\n",
    "        y=y,\n",
    "        k=k,\n",
    "        feature_names=selected_features,\n",
    "        class_labels=[\"No Attrition\", \"Attrition\"],\n",
    "    )\n",
    "\n",
    "# Hide unused axes\n",
    "for ax in axes[len(k_values):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d90525",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn = KNeighborsClassifier(n_neighbors=7)\n",
    "final_knn.fit(X_train_retained, y_train)\n",
    "\n",
    "\n",
    "y_pred = final_knn.predict(X_test_retained)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"No Attrition\", \"Attrition\"],\n",
    "    yticklabels=[\"No Attrition\", \"Attrition\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=[\"No Attrition\", \"Attrition\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ad18b",
   "metadata": {},
   "source": [
    "<!-- train_df = pd.concat() -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psych-723 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
