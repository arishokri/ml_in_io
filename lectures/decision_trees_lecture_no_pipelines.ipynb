{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced9e258",
   "metadata": {},
   "source": [
    "# Decision Trees in scikit-learn\n",
    "\n",
    "This notebook focuses on **implementation** (scikit-learn) for:\n",
    "- classification trees\n",
    "- model evaluation (train/test vs cross-validation)\n",
    "- tuning `max_depth`\n",
    "- bagging and random forests\n",
    "- visualizing decision boundaries and trees\n",
    "- a short regression-tree example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cf7b1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d764f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "RANDOM_STATE = 2025\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "def fit_tabular_preprocessor(X_train: pd.DataFrame):\n",
    "    \"\"\"Fit a simple preprocessor without sklearn preprocessing utilities.\n",
    "\n",
    "    - numeric: fill missing with median\n",
    "    - categorical: fill missing with mode\n",
    "    - one-hot encoding via pandas.get_dummies\n",
    "\n",
    "    Stores the dummy column set learned from training data.\n",
    "    \"\"\"\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "    num_medians = {c: float(X_train[c].median()) for c in num_cols}\n",
    "    cat_modes = {}\n",
    "    for c in cat_cols:\n",
    "        mode_series = X_train[c].mode(dropna=True)\n",
    "        cat_modes[c] = mode_series.iloc[0] if len(mode_series) else \"\"\n",
    "\n",
    "    X_filled = X_train.copy()\n",
    "    for c, med in num_medians.items():\n",
    "        X_filled[c] = X_filled[c].fillna(med)\n",
    "    for c, mode in cat_modes.items():\n",
    "        X_filled[c] = X_filled[c].fillna(mode)\n",
    "\n",
    "    X_dum = pd.get_dummies(X_filled, columns=cat_cols, drop_first=False)\n",
    "    feature_names = X_dum.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"num_cols\": num_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"num_medians\": num_medians,\n",
    "        \"cat_modes\": cat_modes,\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_tabular(X: pd.DataFrame, preproc) -> np.ndarray:\n",
    "    \"\"\"Apply the fitted preprocessor and return a numeric design matrix.\"\"\"\n",
    "    X_filled = X.copy()\n",
    "    for c, med in preproc[\"num_medians\"].items():\n",
    "        if c in X_filled.columns:\n",
    "            X_filled[c] = X_filled[c].fillna(med)\n",
    "    for c, mode in preproc[\"cat_modes\"].items():\n",
    "        if c in X_filled.columns:\n",
    "            X_filled[c] = X_filled[c].fillna(mode)\n",
    "\n",
    "    X_dum = pd.get_dummies(X_filled, columns=preproc[\"cat_cols\"], drop_first=False)\n",
    "    X_dum = X_dum.reindex(columns=preproc[\"feature_names\"], fill_value=0)\n",
    "    return X_dum.values\n",
    "\n",
    "\n",
    "def cv_accuracy(model_ctor, X: pd.DataFrame, y: pd.Series, cv: StratifiedKFold) -> np.ndarray:\n",
    "    \"\"\"Manual CV loop that preprocesses *inside* each fold.\"\"\"\n",
    "    accs = []\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        preproc = fit_tabular_preprocessor(X_train)\n",
    "        X_train_m = transform_tabular(X_train, preproc)\n",
    "        X_test_m = transform_tabular(X_test, preproc)\n",
    "\n",
    "        model = model_ctor()\n",
    "        model.fit(X_train_m, y_train)\n",
    "        y_pred = model.predict(X_test_m)\n",
    "        accs.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return np.array(accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769190bf",
   "metadata": {},
   "source": [
    "## 1) Prepare features and target (classification)\n",
    "We map the target `Attrition` to 0/1, then use a small helper preprocessor that:\n",
    "- fills missing numeric values with the median\n",
    "- fills missing categorical values with the mode\n",
    "- one-hot encodes categoricals with `pandas.get_dummies`\n",
    "\n",
    "(We avoid `SimpleImputer`, `Pipeline`, and `ColumnTransformer`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- verify input dataframe is present ---\n",
    "assert \"df\" in globals(), \"Expected a pandas DataFrame named `df` already loaded in memory.\"\n",
    "\n",
    "# --- basic checks ---\n",
    "assert isinstance(df, pd.DataFrame), \"`df` must be a pandas DataFrame.\"\n",
    "assert \"Attrition\" in df.columns, \"Expected target column `Attrition` in df.\"\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- split X / y ---\n",
    "X = df.drop(columns=[\"Attrition\"]).copy()\n",
    "y_raw = df[\"Attrition\"].copy()\n",
    "\n",
    "# map target to 0/1 if needed\n",
    "if y_raw.dtype == \"O\":\n",
    "    y = y_raw.map({\"No\": 0, \"Yes\": 1})\n",
    "else:\n",
    "    y = y_raw.astype(int)\n",
    "\n",
    "assert set(pd.unique(y)).issubset({0, 1}), \"Target must be binary after mapping.\"\n",
    "\n",
    "# reference: column types\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Rows: {len(X):,}  |  Features: {X.shape[1]:,}  |  Numeric: {len(num_cols)}  |  Categorical: {len(cat_cols)}\")\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13671069",
   "metadata": {},
   "source": [
    "## 2) Accuracy: train/test split vs cross-validation\n",
    "We compare a single train/test split to **10-fold cross-validation**. For the remainder of the notebook we report **cross-validated accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb25c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CV definition (use for the rest of the notebook) ---\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- train/test split accuracy (single estimate) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "preproc_tt = fit_tabular_preprocessor(X_train)\n",
    "X_train_m = transform_tabular(X_train, preproc_tt)\n",
    "X_test_m = transform_tabular(X_test, preproc_tt)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "tree.fit(X_train_m, y_train)\n",
    "split_acc = tree.score(X_test_m, y_test)\n",
    "\n",
    "# --- cross-validated accuracy (more stable estimate) ---\n",
    "cv_scores = cv_accuracy(\n",
    "    model_ctor=lambda: DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "cv_acc = float(cv_scores.mean())\n",
    "\n",
    "print(f\"Train/Test split accuracy: {split_acc:.4f}\")\n",
    "print(f\"10-fold CV accuracy (mean): {cv_acc:.4f}  |  (std): {cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb37cb",
   "metadata": {},
   "source": [
    "## 3) Tune `max_depth` using cross-validation\n",
    "We sweep `max_depth` and plot mean CV accuracy vs depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db92ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = list(range(1, 21))\n",
    "cv_means = []\n",
    "\n",
    "for d in max_depths:\n",
    "    scores = cv_accuracy(\n",
    "        model_ctor=lambda d=d: DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=d),\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "    )\n",
    "    cv_means.append(float(scores.mean()))\n",
    "\n",
    "best_idx = int(np.argmax(cv_means))\n",
    "best_depth = max_depths[best_idx]\n",
    "best_acc = cv_means[best_idx]\n",
    "\n",
    "print(f\"Best max_depth = {best_depth} with mean CV accuracy = {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OO-style plot ---\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_depths, cv_means, marker=\"o\")\n",
    "ax.set_xlabel(\"max_depth\")\n",
    "ax.set_ylabel(\"Mean 10-fold CV Accuracy\")\n",
    "ax.set_title(\"Decision Tree: Accuracy vs max_depth\")\n",
    "ax.set_xticks(max_depths)\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2e529",
   "metadata": {},
   "source": [
    "## 4) Best tree vs ensembles\n",
    "We reuse the best `max_depth` and compare:\n",
    "- single decision tree\n",
    "- bagging\n",
    "- random forest\n",
    "\n",
    "(All with the same CV setup.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e563acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- baseline best-depth tree ---\n",
    "baseline_scores = cv_accuracy(\n",
    "    model_ctor=lambda: DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=best_depth),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "print(f\"Best single tree | mean CV acc: {baseline_scores.mean():.4f} (std {baseline_scores.std():.4f})\")\n",
    "\n",
    "# --- bagging ---\n",
    "bag_scores = cv_accuracy(\n",
    "    model_ctor=lambda: BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=best_depth),\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "print(f\"Bagging          | mean CV acc: {bag_scores.mean():.4f} (std {bag_scores.std():.4f})\")\n",
    "\n",
    "# --- random forest (single configuration) ---\n",
    "rf_scores = cv_accuracy(\n",
    "    model_ctor=lambda: RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        max_depth=best_depth,\n",
    "    ),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "print(f\"Random forest    | mean CV acc: {rf_scores.mean():.4f} (std {rf_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7771ea5",
   "metadata": {},
   "source": [
    "## 5) Random forests: sweep `n_estimators`\n",
    "We vary the number of trees and plot mean CV accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [10, 25, 50, 100, 200, 300, 500]\n",
    "rf_means = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    scores = cv_accuracy(\n",
    "        model_ctor=lambda n_est=n_est: RandomForestClassifier(\n",
    "            n_estimators=n_est,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            max_depth=best_depth,\n",
    "        ),\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "    )\n",
    "    rf_means.append(float(scores.mean()))\n",
    "\n",
    "best_rf_idx = int(np.argmax(rf_means))\n",
    "print(f\"Best n_estimators = {n_estimators_list[best_rf_idx]} with mean CV acc = {rf_means[best_rf_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ad2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(n_estimators_list, rf_means, marker=\"o\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_ylabel(\"Mean 10-fold CV Accuracy\")\n",
    "ax.set_title(\"Random Forest: Accuracy vs n_estimators\")\n",
    "ax.set_xticks(n_estimators_list)\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c1d76",
   "metadata": {},
   "source": [
    "## 6) Decision boundaries with 2 features (classification)\n",
    "To visualize splits, we fit trees using **only two numeric features**.\n",
    "\n",
    "We pick two features (prefer `Age` and `MonthlyIncome` if available), then plot decision boundaries\n",
    "for six different `max_depth` values on a **3×2** grid of subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4634f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- choose two numeric features (prefer common IBM fields) ---\n",
    "preferred = [\"Age\", \"MonthlyIncome\"]\n",
    "two_features = [c for c in preferred if c in X.columns and c in num_cols]\n",
    "\n",
    "if len(two_features) < 2:\n",
    "    # fallback: first two numeric columns\n",
    "    two_features = num_cols[:2]\n",
    "\n",
    "assert len(two_features) == 2, \"Need two numeric features to plot decision boundaries.\"\n",
    "print(\"Using features:\", two_features)\n",
    "\n",
    "X2 = df[two_features].copy()\n",
    "y2 = y.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree_decision_boundary(ax, model, X2: pd.DataFrame, y: pd.Series, title: str):\n",
    "    # mesh\n",
    "    x_min, x_max = X2.iloc[:, 0].min() - 0.5, X2.iloc[:, 0].max() + 0.5\n",
    "    y_min, y_max = X2.iloc[:, 1].min() - 0.5, X2.iloc[:, 1].max() + 0.5\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 300),\n",
    "        np.linspace(y_min, y_max, 300),\n",
    "    )\n",
    "\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    preds = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, preds, alpha=0.25)\n",
    "    ax.scatter(X2.iloc[:, 0], X2.iloc[:, 1], c=y, s=12, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(X2.columns[0])\n",
    "    ax.set_ylabel(X2.columns[1])\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_grid = [1, 2, 3, 4, 6, 10]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, d in zip(axes, depth_grid):\n",
    "    clf2 = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=d)\n",
    "    clf2.fit(X2.values, y2.values)\n",
    "    plot_tree_decision_boundary(ax, clf2, X2, y2, title=f\"max_depth = {d}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af7cf5",
   "metadata": {},
   "source": [
    "## 7) Plot the fitted tree (2 features)\n",
    "We plot a simple decision tree using the same two features as above. We keep it small for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121650e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=3)\n",
    "clf2.fit(X2.values, y2.values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_tree(\n",
    "    clf2,\n",
    "    feature_names=two_features,\n",
    "    class_names=[\"No\", \"Yes\"],\n",
    "    filled=True,\n",
    "    impurity=True,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Decision Tree (2 features, max_depth=3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048376e",
   "metadata": {},
   "source": [
    "## 8) Short regression tree example (built-in dataset)\n",
    "We use scikit-learn's **Diabetes** regression dataset.\n",
    "\n",
    "- Evaluate regression trees with **cross-validated R²**\n",
    "- Sweep `max_depth` and plot mean R²\n",
    "- Visualize a 2-feature regression surface and the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10dfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes(as_frame=True)\n",
    "Xr = diabetes.data\n",
    "yr = diabetes.target\n",
    "\n",
    "# CV for regression\n",
    "cv_r = KFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "depths_r = list(range(1, 16))\n",
    "r2_means = []\n",
    "\n",
    "for d in depths_r:\n",
    "    reg = DecisionTreeRegressor(random_state=RANDOM_STATE, max_depth=d)\n",
    "    scores = cross_val_score(reg, Xr, yr, cv=cv_r, scoring=\"r2\")\n",
    "    r2_means.append(float(scores.mean()))\n",
    "\n",
    "best_r_idx = int(np.argmax(r2_means))\n",
    "best_r_depth = depths_r[best_r_idx]\n",
    "\n",
    "print(f\"Best max_depth = {best_r_depth} with mean CV R² = {r2_means[best_r_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ca127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(depths_r, r2_means, marker=\"o\")\n",
    "ax.set_xlabel(\"max_depth\")\n",
    "ax.set_ylabel(\"Mean 10-fold CV R²\")\n",
    "ax.set_title(\"Regression Tree (Diabetes): R² vs max_depth\")\n",
    "ax.set_xticks(depths_r)\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406346fd",
   "metadata": {},
   "source": [
    "### Regression surface with 2 features\n",
    "We visualize a regression tree trained on **two** features to see piecewise-constant predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1237bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick two features for visualization\n",
    "feat_r = [\"bmi\", \"bp\"]\n",
    "if not all(f in Xr.columns for f in feat_r):\n",
    "    feat_r = Xr.columns[:2].tolist()\n",
    "\n",
    "Xr2 = Xr[feat_r].copy()\n",
    "\n",
    "reg2 = DecisionTreeRegressor(random_state=RANDOM_STATE, max_depth=4)\n",
    "reg2.fit(Xr2.values, yr.values)\n",
    "\n",
    "# mesh\n",
    "x_min, x_max = Xr2.iloc[:, 0].min() - 0.1, Xr2.iloc[:, 0].max() + 0.1\n",
    "y_min, y_max = Xr2.iloc[:, 1].min() - 0.1, Xr2.iloc[:, 1].max() + 0.1\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 250),\n",
    "    np.linspace(y_min, y_max, 250),\n",
    ")\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "preds = reg2.predict(grid).reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cf = ax.contourf(xx, yy, preds, alpha=0.35)\n",
    "ax.scatter(Xr2.iloc[:, 0], Xr2.iloc[:, 1], c=yr, s=12, alpha=0.7)\n",
    "ax.set_xlabel(feat_r[0])\n",
    "ax.set_ylabel(feat_r[1])\n",
    "ax.set_title(\"Regression Tree Predictions (2 features, max_depth=4)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90672cc0",
   "metadata": {},
   "source": [
    "### Plot the regression tree\n",
    "We plot the tree trained on the same two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43502c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plot_tree(\n",
    "    reg2,\n",
    "    feature_names=feat_r,\n",
    "    filled=True,\n",
    "    impurity=True,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Regression Tree (2 features, max_depth=4)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
