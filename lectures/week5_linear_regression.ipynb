{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4efb0ce",
   "metadata": {},
   "source": [
    "# Linear regression: closed form vs. optimization (GD/SGD)\n",
    "\n",
    "This notebook walks through:\n",
    "\n",
    "1. Load a scikit-learn dataset with continuous features  \n",
    "2. Fit a **1D** linear regression line using an **exact (closed-form) solver**  \n",
    "3. Compute **L1** and **L2** losses  \n",
    "4. Plot **loss vs. parameter(s)** to see **V-shape (L1)** and **parabola (L2)** behavior  \n",
    "5. Extend to **multiple features** (design matrix)  \n",
    "6. Solve with **OLS closed form** and **time** it (why it becomes challenging at scale)  \n",
    "7. Solve with **Gradient Descent (GD)** and **Stochastic GD (SGD)** for both 1D and multi-D  \n",
    "8. Plot **loss vs. parameters** (and loss vs. iterations)  \n",
    "9. Visualize the multi-D surface for **two features** (3D + contour), and optionally **animate** GD/SGD paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdc4c0",
   "metadata": {},
   "source": [
    "## Notation (linear algebra)\n",
    "\n",
    "For a dataset with **n** examples and **p** features:\n",
    "\n",
    "- Design matrix:  \n",
    "  $X \\in \\mathbb{R}^{n \\times p}$\n",
    "- Targets:  \n",
    "  $y \\in \\mathbb{R}^{n}$\n",
    "- Linear model (with intercept):\n",
    "  $\\hat y = b\\mathbf{1} + Xw$\n",
    "  where $w \\in \\mathbb{R}^{p}$ and $b \\in \\mathbb{R}$.\n",
    "\n",
    "Common losses:\n",
    "\n",
    "- **L2 / MSE (sum of squares)**:\n",
    "  $\\mathcal{L}_2(w,b) = \\sum_{i=1}^{n} (y_i - (b + x_i^\\top w))^2$\n",
    "- **L1 / MAE (sum of absolute errors)**:\n",
    "  $\\mathcal{L}_1(w,b) = \\sum_{i=1}^{n} \\left|y_i - (b + x_i^\\top w)\\right|$\n",
    "\n",
    "Closed-form **OLS** solution (when \\(X^\\top X\\) is invertible):\n",
    "\n",
    "$w^* = (X^\\top X)^{-1}X^\\top y$\n",
    "\n",
    "A numerically safer equivalent is solving the linear system:\n",
    "$(X^\\top X)w = X^\\top y$\n",
    "without explicitly forming $(X^\\top X)^{-1}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7414db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load a dataset with continuous, standardized features\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data          # already standardized in sklearn diabetes\n",
    "y = diabetes.target.astype(float)\n",
    "\n",
    "feature_names = diabetes.feature_names\n",
    "n, p = X.shape\n",
    "n, p, feature_names[:5], y[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7f9ca",
   "metadata": {},
   "source": [
    "## 2) One feature → one target: fit a line with an exact solver\n",
    "\n",
    "Pick a single feature \\(x \\in \\mathbb{R}^n\\) and fit:\n",
    "\n",
    "\\[\n",
    "\\hat y = b + wx\n",
    "\\]\n",
    "\n",
    "We can solve for \\(w,b\\) by the normal equations in 1D. A convenient way:\n",
    "- augment the design matrix with a column of ones,\n",
    "- solve the 2-parameter OLS problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d13efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one feature (e.g., BMI) and create a 1D design matrix with intercept\n",
    "j = feature_names.index(\"bmi\") if \"bmi\" in feature_names else 2\n",
    "x = X[:, j]  # (n,)\n",
    "\n",
    "X1 = np.column_stack([np.ones(n), x])  # (n,2): [1, x]\n",
    "# Closed-form OLS for (b, w): theta = (X^T X)^{-1} X^T y\n",
    "theta_1d = np.linalg.solve(X1.T @ X1, X1.T @ y)  # safer than explicit inverse\n",
    "b_hat_1d, w_hat_1d = theta_1d\n",
    "\n",
    "b_hat_1d, w_hat_1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fitted line\n",
    "y_hat_1d = X1 @ theta_1d\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, alpha=0.6, label=\"data\")\n",
    "xs = np.linspace(x.min(), x.max(), 200)\n",
    "plt.plot(xs, b_hat_1d + w_hat_1d * xs, linewidth=2, label=\"OLS fit\")\n",
    "plt.xlabel(f\"x = {feature_names[j]}\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.title(\"1D linear regression with closed-form OLS\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7bc46",
   "metadata": {},
   "source": [
    "## 3) Compute L1 and L2 loss (for this fitted line)\n",
    "\n",
    "We'll compute:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_1 = \\sum_i |y_i - \\hat y_i|,\\qquad\n",
    "\\mathcal{L}_2 = \\sum_i (y_i - \\hat y_i)^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_1d = y - y_hat_1d\n",
    "\n",
    "L1_1d = np.sum(np.abs(resid_1d))\n",
    "L2_1d = np.sum(resid_1d**2)\n",
    "\n",
    "L1_1d, L2_1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad1fe6",
   "metadata": {},
   "source": [
    "## 4) Plot L1 and L2 vs parameters (show V-shape vs parabola)\n",
    "\n",
    "To make the shapes clear, we will plot the loss as a function of **a single parameter** $w$.\n",
    "To keep it 1D, we temporarily fix the intercept $b$ to the OLS value $b=\\hat b$, and vary $w$.\n",
    "\n",
    "$\\mathcal{L}_1(w) = \\sum_i |y_i - (\\hat b + w x_i)|,\\qquad\n",
    "\\mathcal{L}_2(w) = \\sum_i (y_i - (\\hat b + w x_i))^2$\n",
    "\n",
    "- $\\mathcal{L}_1(w)$ is **piecewise linear** → \"V-ish\" around its minimum  \n",
    "- $\\mathcal{L}_2(w)$ is **quadratic** → parabola around its minimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep w around the OLS solution and compute L1/L2\n",
    "w_grid = np.linspace(w_hat_1d - 80, w_hat_1d + 80, 600)\n",
    "\n",
    "L1_grid = []\n",
    "L2_grid = []\n",
    "for w in w_grid:\n",
    "    y_hat = b_hat_1d + w * x\n",
    "    r = y - y_hat\n",
    "    L1_grid.append(np.sum(np.abs(r)))\n",
    "    L2_grid.append(np.sum(r**2))\n",
    "\n",
    "L1_grid = np.array(L1_grid)\n",
    "L2_grid = np.array(L2_grid)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_grid, L1_grid, label=\"L1 loss (sum abs)\")\n",
    "ax.axvline(w_hat_1d, linestyle=\"--\", label=\"ŵ (OLS)\")\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"L1 loss vs parameter w (V-shaped / piecewise linear)\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_grid, L2_grid, label=\"L2 loss (sum squares)\")\n",
    "ax.axvline(w_hat_1d, linestyle=\"--\", label=\"ŵ (OLS)\")\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"L2 loss vs parameter w (parabola / quadratic)\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e0400",
   "metadata": {},
   "source": [
    "## 5) Multiple features → design matrix\n",
    "\n",
    "Now we use multiple features and solve:\n",
    "\n",
    "$\\hat y = b + Xw$\n",
    "\n",
    "We'll use **all features** from the diabetes dataset first, then later pick only 2 features for a 3D visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db860162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept column for multi-feature OLS\n",
    "X_full = np.column_stack([np.ones(n), X])  # (n, p+1); parameters theta = [b, w1, ..., wp]\n",
    "\n",
    "# Closed-form OLS solve: (X^T X) theta = X^T y\n",
    "theta_full = np.linalg.solve(X_full.T @ X_full, X_full.T @ y)\n",
    "b_hat_full, w_hat_full = theta_full[0], theta_full[1:]\n",
    "\n",
    "b_hat_full, w_hat_full[:5], w_hat_full.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5ac26",
   "metadata": {},
   "source": [
    "## 6) Timing OLS closed-form (why it can be challenging)\n",
    "\n",
    "The bottleneck in closed-form OLS is typically forming and solving the **normal equations**:\n",
    "\n",
    "- Compute $X^\\top X:O(np^2)$\n",
    "- Solve a $p \\times p$ system: roughly $O(p^3)$\n",
    "\n",
    "For small \\(p\\) it's fast, but as \\(p\\) grows, \\(p^3\\) gets expensive.\n",
    "\n",
    "Below we create **synthetic high-dimensional** feature matrices (still derived from the dataset) to show the cost scaling.\n",
    "We also add a small ridge term $\\lambda I$ to make the system well-conditioned for inversion/solve:\n",
    "\n",
    "$(X^\\top X + \\lambda I)\\theta = X^\\top y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_closed_form(n=442, p=50, ridge=1e-6, repeats=3):\n",
    "    # synthetic X: random projection of original X to p dims\n",
    "    # (so we're not just padding zeros)\n",
    "    X0 = X  # (n,10)\n",
    "    Wproj = np.random.randn(X0.shape[1], p)\n",
    "    Xp = X0 @ Wproj  # (n,p)\n",
    "\n",
    "    X_aug = np.column_stack([np.ones(n), Xp])  # (n, p+1)\n",
    "    A = X_aug.T @ X_aug\n",
    "    A = A + ridge * np.eye(A.shape[0])\n",
    "    b = X_aug.T @ y\n",
    "\n",
    "    # time solve\n",
    "    times = []\n",
    "    for _ in range(repeats):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = np.linalg.solve(A, b)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    return float(np.mean(times)), float(np.std(times))\n",
    "\n",
    "dims = [10, 50, 100, 200, 400, 800]\n",
    "timings = []\n",
    "for p_dim in dims:\n",
    "    mu, sd = time_closed_form(n=n, p=p_dim, ridge=1e-6, repeats=5)\n",
    "    timings.append((p_dim, mu, sd))\n",
    "\n",
    "timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e22c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timing vs dimension\n",
    "dims_arr = np.array([t[0] for t in timings])\n",
    "mu_arr = np.array([t[1] for t in timings])\n",
    "sd_arr = np.array([t[2] for t in timings])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dims_arr, mu_arr, marker=\"o\")\n",
    "plt.fill_between(dims_arr, mu_arr - sd_arr, mu_arr + sd_arr, alpha=0.2)\n",
    "plt.xlabel(\"Number of synthetic features p\")\n",
    "plt.ylabel(\"Solve time (seconds)\")\n",
    "plt.title(\"Closed-form solve time grows quickly with p (normal equations)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c695a",
   "metadata": {},
   "source": [
    "## 7) Gradient Descent (GD) and Stochastic GD (SGD)\n",
    "\n",
    "We'll optimize the **MSE objective** (mean squared error), which is differentiable:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{n}\\|y - X\\theta\\|_2^2 = \\frac{1}{n}(y - X\\theta)^\\top (y - X\\theta)\n",
    "\\]\n",
    "\n",
    "Gradient:\n",
    "\\[\n",
    "\\nabla_\\theta J(\\theta) = -\\frac{2}{n}X^\\top (y - X\\theta)\n",
    "\\]\n",
    "\n",
    "Update rule (GD):\n",
    "\\[\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta J(\\theta^{(t)})\n",
    "\\]\n",
    "\n",
    "For SGD, we use a (mini)batch of size \\(m\\) (often 1):\n",
    "\\[\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\left(-\\frac{2}{m}X_B^\\top (y_B - X_B\\theta^{(t)})\\right)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c82206",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GDHistory:\n",
    "    thetas: list\n",
    "    losses: list\n",
    "\n",
    "def mse_loss(Xa, y, theta):\n",
    "    r = y - Xa @ theta\n",
    "    return float(np.mean(r**2))\n",
    "\n",
    "def gd_mse(Xa, y, lr=0.05, steps=200, theta0=None):\n",
    "    n = Xa.shape[0]\n",
    "    theta = np.zeros(Xa.shape[1]) if theta0 is None else theta0.astype(float).copy()\n",
    "    hist = GDHistory(thetas=[theta.copy()], losses=[mse_loss(Xa, y, theta)])\n",
    "    for _ in range(steps):\n",
    "        r = y - Xa @ theta\n",
    "        grad = -(2/n) * (Xa.T @ r)\n",
    "        theta = theta - lr * grad\n",
    "        hist.thetas.append(theta.copy())\n",
    "        hist.losses.append(mse_loss(Xa, y, theta))\n",
    "    return hist\n",
    "\n",
    "def sgd_mse(Xa, y, lr=0.05, steps=500, batch_size=1, theta0=None, shuffle=True):\n",
    "    n = Xa.shape[0]\n",
    "    theta = np.zeros(Xa.shape[1]) if theta0 is None else theta0.astype(float).copy()\n",
    "    hist = GDHistory(thetas=[theta.copy()], losses=[mse_loss(Xa, y, theta)])\n",
    "    idx = np.arange(n)\n",
    "    for t in range(steps):\n",
    "        if shuffle and (t % n == 0):\n",
    "            np.random.shuffle(idx)\n",
    "        # mini-batch\n",
    "        start = (t * batch_size) % n\n",
    "        batch_idx = idx[start:start+batch_size]\n",
    "        Xb = Xa[batch_idx]\n",
    "        yb = y[batch_idx]\n",
    "        rb = yb - Xb @ theta\n",
    "        grad = -(2/len(batch_idx)) * (Xb.T @ rb)\n",
    "        theta = theta - lr * grad\n",
    "        hist.thetas.append(theta.copy())\n",
    "        hist.losses.append(mse_loss(Xa, y, theta))  # track full-dataset loss for comparability\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2f5d9",
   "metadata": {},
   "source": [
    "### 7A) Solve the 1D problem with GD and SGD\n",
    "\n",
    "We'll reuse the 1D augmented matrix \\(X_1 = [\\mathbf{1}, x]\\) and compare to the OLS solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D optimization\n",
    "hist_gd_1d = gd_mse(X1, y, lr=0.2, steps=80)\n",
    "hist_sgd_1d = sgd_mse(X1, y, lr=0.05, steps=1200, batch_size=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_gd_1d.losses, label=\"GD (full batch)\")\n",
    "plt.plot(hist_sgd_1d.losses, label=\"SGD (batch=1)\", alpha=0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE (log scale)\")\n",
    "plt.title(\"1D: GD vs SGD convergence (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare final parameters\n",
    "theta_gd_1d = hist_gd_1d.thetas[-1]\n",
    "theta_sgd_1d = hist_sgd_1d.thetas[-1]\n",
    "theta_ols_1d = theta_1d\n",
    "\n",
    "theta_ols_1d, theta_gd_1d, theta_sgd_1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac225d0",
   "metadata": {},
   "source": [
    "## 8–9) Graph the loss vs parameters (1D is easy)\n",
    "\n",
    "For the 1D model (with intercept), parameters are \\((b,w)\\).  \n",
    "We can visualize the loss surface by making a grid over \\(b\\) and \\(w\\) and plotting contours.\n",
    "\n",
    "This also lets us overlay the **optimization trajectories** for GD and SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a grid over (b, w) and compute MSE\n",
    "b_grid = np.linspace(b_hat_1d - 80, b_hat_1d + 80, 200)\n",
    "w_grid2 = np.linspace(w_hat_1d - 80, w_hat_1d + 80, 200)\n",
    "\n",
    "BB, WW = np.meshgrid(b_grid, w_grid2)\n",
    "Loss = np.zeros_like(BB)\n",
    "\n",
    "for i in range(BB.shape[0]):\n",
    "    b0 = BB[i]\n",
    "    w0 = WW[i]\n",
    "    y_hat = b0 + w0 * x\n",
    "    Loss[i] = np.mean((y - y_hat)**2)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "cs = plt.contour(BB, WW, Loss, levels=30)\n",
    "plt.clabel(cs, inline=True, fontsize=8)\n",
    "plt.xlabel(\"b (intercept)\")\n",
    "plt.ylabel(\"w (slope)\")\n",
    "plt.title(\"1D MSE loss contours over (b, w)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# overlay paths\n",
    "gd_path = np.array(hist_gd_1d.thetas)  # (T,2) with [b,w]\n",
    "sgd_path = np.array(hist_sgd_1d.thetas)\n",
    "\n",
    "plt.plot(gd_path[:,0], gd_path[:,1], marker=\"o\", markersize=2, linewidth=1.5, label=\"GD path\")\n",
    "plt.plot(sgd_path[::20,0], sgd_path[::20,1], marker=\"x\", markersize=3, linewidth=1.0, label=\"SGD path (every 20 steps)\")\n",
    "\n",
    "plt.scatter([theta_ols_1d[0]], [theta_ols_1d[1]], s=80, label=\"OLS optimum\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467c56d",
   "metadata": {},
   "source": [
    "## 10) Multi-dimensional: visualize only 2 features\n",
    "\n",
    "With \\(p>2\\), the loss surface lives in high dimensions and can't be fully graphed.\n",
    "\n",
    "A common trick is to:\n",
    "- pick 2 features (\\(w_1,w_2\\)),\n",
    "- optionally fix the intercept and other weights,\n",
    "- then visualize the surface over \\((w_1,w_2)\\).\n",
    "\n",
    "We'll pick two features and fit a model with intercept + these two weights:\n",
    "\\[\n",
    "\\hat y = b + w_1 x_{(1)} + w_2 x_{(2)}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick two features for visualization\n",
    "feat_idx = [feature_names.index(\"bmi\") if \"bmi\" in feature_names else 2,\n",
    "            feature_names.index(\"bp\")  if \"bp\"  in feature_names else 3]\n",
    "f1, f2 = feat_idx\n",
    "X2 = X[:, [f1, f2]]\n",
    "X2_aug = np.column_stack([np.ones(n), X2])  # (n,3): [1, x1, x2]\n",
    "\n",
    "theta_ols_2 = np.linalg.solve(X2_aug.T @ X2_aug, X2_aug.T @ y)\n",
    "theta_ols_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GD/SGD on this 2-feature problem\n",
    "hist_gd_2 = gd_mse(X2_aug, y, lr=0.2, steps=120)\n",
    "hist_sgd_2 = sgd_mse(X2_aug, y, lr=0.03, steps=2500, batch_size=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_gd_2.losses, label=\"GD\")\n",
    "plt.plot(hist_sgd_2.losses, label=\"SGD\", alpha=0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE (log scale)\")\n",
    "plt.title(\"2-feature model: GD vs SGD convergence\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9b32b",
   "metadata": {},
   "source": [
    "### 3D surface (optional) + contour with optimization paths\n",
    "\n",
    "We'll visualize the MSE as a function of \\((w_1,w_2)\\) while fixing \\(b\\) to its OLS value \\(\\hat b\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd834718",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_fix = float(theta_ols_2[0])\n",
    "\n",
    "w1_grid = np.linspace(theta_ols_2[1]-120, theta_ols_2[1]+120, 120)\n",
    "w2_grid = np.linspace(theta_ols_2[2]-120, theta_ols_2[2]+120, 120)\n",
    "W1, W2 = np.meshgrid(w1_grid, w2_grid)\n",
    "\n",
    "Z = np.zeros_like(W1)\n",
    "x1 = X2[:, 0]\n",
    "x2 = X2[:, 1]\n",
    "for i in range(W1.shape[0]):\n",
    "    y_hat = b_fix + W1[i]*x1 + W2[i]*x2\n",
    "    Z[i] = np.mean((y - y_hat)**2)\n",
    "\n",
    "# Contour plot + paths projected to (w1,w2)\n",
    "gd_path2 = np.array(hist_gd_2.thetas)[:, 1:3]\n",
    "sgd_path2 = np.array(hist_sgd_2.thetas)[:, 1:3]\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "cs = plt.contour(W1, W2, Z, levels=30)\n",
    "plt.clabel(cs, inline=True, fontsize=8)\n",
    "plt.xlabel(f\"w1 for {feature_names[f1]}\")\n",
    "plt.ylabel(f\"w2 for {feature_names[f2]}\")\n",
    "plt.title(\"2-feature MSE contours (b fixed) with GD/SGD paths\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(gd_path2[:,0], gd_path2[:,1], marker=\"o\", markersize=2, linewidth=1.5, label=\"GD path\")\n",
    "plt.plot(sgd_path2[::30,0], sgd_path2[::30,1], marker=\"x\", markersize=3, linewidth=1.0, label=\"SGD path (every 30 steps)\")\n",
    "plt.scatter([theta_ols_2[1]], [theta_ols_2[2]], s=80, label=\"OLS optimum (w1,w2)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312882a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D surface plot (can be heavy; keep grid moderate)\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(W1, W2, Z, rstride=2, cstride=2, linewidth=0, antialiased=True, alpha=0.8)\n",
    "ax.set_xlabel(f\"w1 ({feature_names[f1]})\")\n",
    "ax.set_ylabel(f\"w2 ({feature_names[f2]})\")\n",
    "ax.set_zlabel(\"MSE (b fixed)\")\n",
    "ax.set_title(\"2-feature MSE surface (3D)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05323e",
   "metadata": {},
   "source": [
    "## 11) Optional: animate GD vs SGD path on the contour (GIF)\n",
    "\n",
    "If `Pillow` is available, we can save an animated GIF that shows the optimization path.\n",
    "\n",
    "If this fails in your environment, you can skip this cell and rely on the static contour plots above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create a GIF animation of the paths (contour + moving points)\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "gif_path = \"gd_vs_sgd_paths.gif\"\n",
    "\n",
    "# Reduce the number of frames for a lighter GIF\n",
    "gd_frames = gd_path2\n",
    "sgd_frames = sgd_path2[::10]  # downsample SGD\n",
    "\n",
    "T = max(len(gd_frames), len(sgd_frames))\n",
    "# pad shorter path by repeating last point\n",
    "if len(gd_frames) < T:\n",
    "    gd_frames = np.vstack([gd_frames, np.repeat(gd_frames[-1][None,:], T-len(gd_frames), axis=0)])\n",
    "if len(sgd_frames) < T:\n",
    "    sgd_frames = np.vstack([sgd_frames, np.repeat(sgd_frames[-1][None,:], T-len(sgd_frames), axis=0)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "cs = ax.contour(W1, W2, Z, levels=30)\n",
    "ax.clabel(cs, inline=True, fontsize=8)\n",
    "ax.set_xlabel(f\"w1 for {feature_names[f1]}\")\n",
    "ax.set_ylabel(f\"w2 for {feature_names[f2]}\")\n",
    "ax.set_title(\"GD vs SGD path (b fixed)\")\n",
    "\n",
    "# Static optimum marker\n",
    "ax.scatter([theta_ols_2[1]], [theta_ols_2[2]], s=80, label=\"OLS optimum\")\n",
    "\n",
    "# Lines + moving points\n",
    "(line_gd,) = ax.plot([], [], linewidth=1.5, label=\"GD\")\n",
    "(line_sgd,) = ax.plot([], [], linewidth=1.0, label=\"SGD\")\n",
    "(pt_gd,) = ax.plot([], [], marker=\"o\", markersize=5, linestyle=\"None\")\n",
    "(pt_sgd,) = ax.plot([], [], marker=\"x\", markersize=6, linestyle=\"None\")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "def init():\n",
    "    line_gd.set_data([], [])\n",
    "    line_sgd.set_data([], [])\n",
    "    pt_gd.set_data([], [])\n",
    "    pt_sgd.set_data([], [])\n",
    "    return line_gd, line_sgd, pt_gd, pt_sgd\n",
    "\n",
    "def update(frame):\n",
    "    line_gd.set_data(gd_frames[:frame+1,0], gd_frames[:frame+1,1])\n",
    "    line_sgd.set_data(sgd_frames[:frame+1,0], sgd_frames[:frame+1,1])\n",
    "    pt_gd.set_data([gd_frames[frame,0]], [gd_frames[frame,1]])\n",
    "    pt_sgd.set_data([sgd_frames[frame,0]], [sgd_frames[frame,1]])\n",
    "    return line_gd, line_sgd, pt_gd, pt_sgd\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, init_func=init, frames=T, interval=30, blit=True)\n",
    "\n",
    "# Save GIF\n",
    "try:\n",
    "    writer = animation.PillowWriter(fps=25)\n",
    "    anim.save(gif_path, writer=writer)\n",
    "    print(f\"Saved GIF to: {gif_path}\")\n",
    "except Exception as e:\n",
    "    print(\"GIF save failed:\", repr(e))\n",
    "    print(\"You can still use the static plots above.\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7f2c7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Closed-form OLS gives an exact solution (when feasible) but can become expensive as feature dimension \\(p\\) grows.\n",
    "- GD/SGD avoid matrix inversion; they can scale better to large \\(p\\) and large \\(n\\), but require:\n",
    "  - learning rate tuning,\n",
    "  - iteration budgeting,\n",
    "  - and careful monitoring of convergence.\n",
    "\n",
    "You now have:\n",
    "- L1 vs L2 loss curves (V-shape vs parabola)\n",
    "- Closed-form OLS solutions (1D and multi-D)\n",
    "- GD/SGD implementations from scratch\n",
    "- Loss vs iterations plots\n",
    "- Loss vs parameter(s) plots (contours / surfaces)\n",
    "- Optional GIF animation of optimization paths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psych-723 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
